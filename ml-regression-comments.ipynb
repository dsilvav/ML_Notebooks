{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nav_menu":{"height":"279px","width":"309px"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false},"colab":{"provenance":[{"file_id":"https://github.com/marcoteran/ml/blob/master/notebooks/ml_regression.ipynb","timestamp":1760642593996}]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/marcoteran/ml/blob/master/notebooks/ml_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/ml/blob/master/notebooks/ml_regression.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n  </td>\n</table>","metadata":{"id":"I1ynyuVMeF8i"}},{"cell_type":"markdown","source":"# Sesi√≥n 02: Regresi√≥n Lineal y Log√≠stica\n## Gu√≠a Completa\n\n**Machine Learning**\n\n**Profesor:** Marco Ter√°n  \n**Fecha:** 2025\n\n[Website](http://marcoteran.github.io/),\n[Github](https://github.com/marcoteran),\n[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n___","metadata":{"id":"OxaE9Nt3eF8k"}},{"cell_type":"markdown","source":"## üìã Tabla de Contenidos\n\n1. **[Introducci√≥n y Setup](#1-introducci√≥n-y-setup)**\n2. **[Regresi√≥n Lineal Simple](#2-regresi√≥n-lineal-simple)**\n3. **[Regresi√≥n Lineal M√∫ltiple](#3-regresi√≥n-lineal-m√∫ltiple)**\n4. **[Gradient Descent desde Cero](#4-gradient-descent-desde-cero)**\n6. **[Regularizaci√≥n](#6-regularizaci√≥n)**\n7. **[Regresi√≥n Log√≠stica](#7-regresi√≥n-log√≠stica)**\n8. **[M√©tricas de Clasificaci√≥n](#8-m√©tricas-de-clasificaci√≥n)**\n9. **[Casos Pr√°cticos Reales](#9-casos-pr√°cticos-reales)**\n10. **[Proyecto Final Integrador](#10-proyecto-final-integrador)**\n\n---","metadata":{"id":"rE29y2SceF8k"}},{"cell_type":"markdown","source":"## 1. Introducci√≥n y Setup\n\n### üìñ Teor√≠a: ¬øQu√© es la Regresi√≥n?\n\nLa **regresi√≥n** es una t√©cnica de aprendizaje supervisado que busca modelar la relaci√≥n entre variables. Existen dos tipos principales:\n\n- **Regresi√≥n Lineal**: Predice valores continuos (precios, temperaturas, ventas)\n- **Regresi√≥n Log√≠stica**: Predice probabilidades/categor√≠as (spam/no spam, aprobado/rechazado)","metadata":{"id":"x8EqyzBMeF8l"}},{"cell_type":"markdown","source":"---\n\n## üîß Instalaci√≥n y Configuraci√≥n del Entorno","metadata":{"id":"5qGoikLeeF8l"}},{"cell_type":"code","source":"# Importar todas las librer√≠as necesarias\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Librer√≠as de Machine Learning\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet,\n                                  LogisticRegression, RidgeCV, LassoCV)\nfrom sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n                           accuracy_score, precision_score, recall_score, f1_score,\n                           confusion_matrix, roc_curve, auc, roc_auc_score, classification_report)\nfrom sklearn.datasets import load_diabetes, load_breast_cancer, load_iris\n\n# Configuraci√≥n de visualizaci√≥n\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\n# Configuraci√≥n de pandas\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Semilla para reproducibilidad\nnp.random.seed(42)\n\nprint(\"‚úÖ Librer√≠as cargadas exitosamente\")\nprint(f\"üìä Versiones:\")\nprint(f\"  ‚Ä¢ NumPy: {np.__version__}\")\nprint(f\"  ‚Ä¢ Pandas: {pd.__version__}\")\nprint(f\"  ‚Ä¢ Scikit-learn: {sklearn.__version__}\")","metadata":{"id":"xvdxZo9deF8l","executionInfo":{"status":"ok","timestamp":1760642636217,"user_tz":300,"elapsed":6186,"user":{"displayName":"David Fonseca","userId":"10042764328509696167"}},"outputId":"2ec850e9-9cda-43f1-af24-03127ba067cc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It also requires Scikit-Learn ‚â• 1.0.1:","metadata":{"id":"a800kJveeF8m"}},{"cell_type":"markdown","source":"**COMENTARIO:**\nPreparaci√≥n del entorno:\nEste bloque configura el entorno de trabajo cargando librer√≠as clave para an√°lisis, visualizaci√≥n y machine learning. Se fija la semilla con np.random.seed(42) para garantizar reproducibilidad, una pr√°ctica esencial en experimentos. Aqu√≠ se reflejan buenas pr√°cticas de inicializaci√≥n y preparaci√≥n de notebooks.","metadata":{"id":"Xu8_12O1lnMj"}},{"cell_type":"markdown","source":"### üé® Funciones de Utilidad para Visualizaci√≥n","metadata":{"id":"PbkghpoDeF8m"}},{"cell_type":"code","source":"# Paleta de colores consistente\nCOLORS = {\n    'primary': '#2E86AB',\n    'secondary': '#A23B72',\n    'accent': '#F18F01',\n    'success': '#73AB84',\n    'warning': '#C73E1D',\n    'dark': '#2D3142',\n    'light': '#F0F0F0'\n}\n\ndef plot_style(ax, title=\"\", xlabel=\"\", ylabel=\"\"):\n    \"\"\"Aplicar estilo consistente a los gr√°ficos\"\"\"\n    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n    ax.set_xlabel(xlabel, fontsize=12)\n    ax.set_ylabel(ylabel, fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\ndef print_section(title):\n    \"\"\"Imprimir t√≠tulo de secci√≥n con estilo\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(f\"  {title}\")\n    print(\"=\"*60 + \"\\n\")","metadata":{"id":"92eYZKjCeF8n","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:46:16.946927Z","iopub.execute_input":"2025-11-04T04:46:16.947257Z","iopub.status.idle":"2025-11-04T04:46:16.958147Z","shell.execute_reply.started":"2025-11-04T04:46:16.947225Z","shell.execute_reply":"2025-11-04T04:46:16.956976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:**\nPaleta y estilo gr√°fico:\nEn este bloque se define una paleta de colores personalizada para reutilizar en todas las visualizaciones. Destaca la funci√≥n plot_style, que estandariza t√≠tulos y ejes, evitando repetir c√≥digo y asegurando consistencia visual.","metadata":{"id":"tsW5EMVamOvK"}},{"cell_type":"markdown","source":"---\n\n## 2. Regresi√≥n Lineal Simple\n\n### üìñ Teor√≠a: Modelo Lineal Simple\n\nEl modelo de regresi√≥n lineal simple busca una relaci√≥n lineal entre una variable independiente $x$ y una variable dependiente $y$:\n\n$$y = \\theta_0 + \\theta_1 x + \\epsilon$$\n\nDonde:\n- $\\theta_0$: Intercepto (bias)\n- $\\theta_1$: Pendiente (peso)\n- $\\epsilon$: Error aleatorio","metadata":{"id":"AMnxqaTneF8n"}},{"cell_type":"markdown","source":"### üíª Implementaci√≥n desde Cero","metadata":{"id":"DNOn4BoUeF8n"}},{"cell_type":"code","source":"print_section(\"2.1 REGRESI√ìN LINEAL SIMPLE - IMPLEMENTACI√ìN MANUAL\")\n\nclass SimpleLinearRegression:\n    \"\"\"\n    Implementaci√≥n desde cero de Regresi√≥n Lineal Simple\n    usando la ecuaci√≥n normal (m√©todo de m√≠nimos cuadrados)\n    \"\"\"\n\n    def __init__(self):\n        self.theta_0 = None  # Intercepto\n        self.theta_1 = None  # Pendiente\n        self.r_squared = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Ajustar el modelo usando la ecuaci√≥n normal\n        Œ∏‚ÇÅ = Œ£((x - xÃÑ)(y - »≥)) / Œ£((x - xÃÑ)¬≤)\n        Œ∏‚ÇÄ = »≥ - Œ∏‚ÇÅxÃÑ\n        \"\"\"\n        X = np.array(X).flatten()\n        y = np.array(y).flatten()\n\n        # Calcular medias\n        x_mean = np.mean(X)\n        y_mean = np.mean(y)\n\n        # Calcular pendiente\n        numerator = np.sum((X - x_mean) * (y - y_mean))\n        denominator = np.sum((X - x_mean) ** 2)\n        self.theta_1 = numerator / denominator\n\n        # Calcular intercepto\n        self.theta_0 = y_mean - self.theta_1 * x_mean\n\n        # Calcular R¬≤\n        y_pred = self.predict(X)\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - y_mean) ** 2)\n        self.r_squared = 1 - (ss_res / ss_tot)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Realizar predicciones\"\"\"\n        X = np.array(X).flatten()\n        return self.theta_0 + self.theta_1 * X\n\n    def get_equation(self):\n        \"\"\"Obtener ecuaci√≥n del modelo\"\"\"\n        return f\"y = {self.theta_0:.2f} + {self.theta_1:.2f}x\"\n\n    def plot_fit(self, X, y, title=\"Regresi√≥n Lineal Simple\"):\n        \"\"\"Visualizar el ajuste del modelo\"\"\"\n        X = np.array(X).flatten()\n        y = np.array(y).flatten()\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n        # Gr√°fico de ajuste\n        axes[0].scatter(X, y, alpha=0.6, s=50, color=COLORS['accent'],\n                       edgecolors='black', linewidth=0.5, label='Datos reales')\n\n        # L√≠nea de regresi√≥n\n        X_line = np.linspace(X.min(), X.max(), 100)\n        y_pred_line = self.predict(X_line)\n        axes[0].plot(X_line, y_pred_line, color=COLORS['primary'],\n                    linewidth=2, label=self.get_equation())\n\n        # Residuos\n        y_pred = self.predict(X)\n        for xi, yi, yi_pred in zip(X, y, y_pred):\n            axes[0].plot([xi, xi], [yi, yi_pred], 'k--', alpha=0.3, linewidth=0.8)\n\n        plot_style(axes[0], title, \"X\", \"Y\")\n        axes[0].legend()\n\n        # A√±adir m√©tricas\n        mse = np.mean((y - y_pred) ** 2)\n        mae = np.mean(np.abs(y - y_pred))\n        axes[0].text(0.05, 0.95, f'R¬≤ = {self.r_squared:.4f}\\nMSE = {mse:.4f}\\nMAE = {mae:.4f}',\n                    transform=axes[0].transAxes, verticalalignment='top',\n                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\n        # Gr√°fico de residuos\n        residuals = y - y_pred\n        axes[1].scatter(y_pred, residuals, alpha=0.6, s=50,\n                       color=COLORS['secondary'], edgecolors='black', linewidth=0.5)\n        axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n        axes[1].fill_between([y_pred.min(), y_pred.max()],\n                            [-2*residuals.std(), -2*residuals.std()],\n                            [2*residuals.std(), 2*residuals.std()],\n                            alpha=0.2, color='gray', label='¬±2œÉ')\n\n        plot_style(axes[1], \"An√°lisis de Residuos\", \"Valores Predichos\", \"Residuos\")\n        axes[1].legend()\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"id":"sIAGnVLeeF8n"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque implementa una regresi√≥n lineal simple desde las f√≥rmulas de m√≠nimos cuadrados: se calculan pendiente e intercepto con medias y covarianzas, se predicen valores y se eval√∫a el desempe√±o con R¬≤, MSE y MAE. Los gr√°ficos de ajuste y residuos ayudan a validar supuestos (linealidad, homocedasticidad, ausencia de patrones). Tambi√©n permite detectar problemas como baja variabilidad en X, que puede causar divisi√≥n por cero.","metadata":{"id":"63yWR7AGnLjI"}},{"cell_type":"code","source":"# Ejemplo con datos sint√©ticos\nprint(\"üìä Generando datos sint√©ticos para demostraci√≥n...\")\nnp.random.seed(42)\nX_simple = np.random.uniform(0, 10, 100)\ny_simple = 2.5 * X_simple + 10 + np.random.normal(0, 2, 100)\n\n# Entrenar modelo\nmodel_simple = SimpleLinearRegression()\nmodel_simple.fit(X_simple, y_simple)\n\nprint(f\"‚úÖ Modelo entrenado:\")\nprint(f\"   Ecuaci√≥n: {model_simple.get_equation()}\")\nprint(f\"   R¬≤: {model_simple.r_squared:.4f}\")\n\n# Visualizar\nmodel_simple.plot_fit(X_simple, y_simple, \"Regresi√≥n Lineal Simple - Datos Sint√©ticos\")","metadata":{"id":"mSzCU0R9eF8o"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** \nDatos sint√©ticos y validaci√≥n:\nAqu√≠ se generan datos sint√©ticos con relaci√≥n lineal conocida (pendiente ‚âà 2.5, intercepto ‚âà 10) m√°s ruido gaussiano. Se fija la semilla para reproducibilidad y se entrena SimpleLinearRegression para comprobar si recupera los par√°metros. Revisar la ecuaci√≥n estimada y R¬≤ permite medir la cercan√≠a a la verdad y entender el impacto del ruido, mientras que las visualizaciones validan supuestos b√°sicos.\n\n\n","metadata":{"id":"3AJhpuLhn5hZ"}},{"cell_type":"markdown","source":"### üìä Comparaci√≥n con Scikit-learn","metadata":{"id":"6B0xJ3vkeF8o"}},{"cell_type":"code","source":"print_section(\"2.2 COMPARACI√ìN CON SCIKIT-LEARN\")\n\n# Usar scikit-learn\nfrom sklearn.linear_model import LinearRegression\n\n# Preparar datos (sklearn espera matrices 2D)\nX_sklearn = X_simple.reshape(-1, 1)\n\n# Entrenar modelo\nmodel_sklearn = LinearRegression()\nmodel_sklearn.fit(X_sklearn, y_simple)\n\n# Comparar resultados\nprint(\"üìä Comparaci√≥n de resultados:\")\nprint(f\"{'M√©todo':<20} {'Œ∏‚ÇÄ (Intercepto)':<15} {'Œ∏‚ÇÅ (Pendiente)':<15} {'R¬≤':<10}\")\nprint(\"-\" * 60)\nprint(f\"{'Implementaci√≥n':<20} {model_simple.theta_0:<15.4f} {model_simple.theta_1:<15.4f} {model_simple.r_squared:<10.4f}\")\nprint(f\"{'Scikit-learn':<20} {model_sklearn.intercept_:<15.4f} {model_sklearn.coef_[0]:<15.4f} {model_sklearn.score(X_sklearn, y_simple):<10.4f}\")\nprint(\"\\n‚úÖ Los resultados son id√©nticos, validando nuestra implementaci√≥n!\")","metadata":{"id":"euUxMKHfeF8o"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** \nEste bloque muestra c√≥mo los valores at√≠picos afectan la regresi√≥n lineal. En tres escenarios se observa c√≥mo cambian la pendiente y R¬≤ al a√±adir puntos extremos. Se evidencia la sensibilidad del modelo ante datos an√≥malos y se mencionan alternativas robustas como RANSAC y Huber.\n\n\n","metadata":{"id":"bPUxT3qHotxT"}},{"cell_type":"markdown","source":"### üéØ Ejercicio Interactivo: Efecto de Outliers","metadata":{"id":"5tcW_y-EeF8o"}},{"cell_type":"code","source":"print_section(\"2.3 EFECTO DE OUTLIERS EN REGRESI√ìN LINEAL\")\n\ndef demonstrate_outlier_effect():\n    \"\"\"Demostrar c√≥mo los outliers afectan la regresi√≥n lineal\"\"\"\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Datos originales\n    np.random.seed(42)\n    X = np.random.uniform(0, 10, 50)\n    y = 2 * X + 5 + np.random.normal(0, 1, 50)\n\n    # Sin outliers\n    model1 = LinearRegression()\n    model1.fit(X.reshape(-1, 1), y)\n    axes[0].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n    axes[0].plot(X, model1.predict(X.reshape(-1, 1)),\n                color=COLORS['primary'], linewidth=2)\n    plot_style(axes[0], \"Sin Outliers\", \"X\", \"Y\")\n    axes[0].text(0.05, 0.95, f'R¬≤ = {model1.score(X.reshape(-1, 1), y):.3f}',\n                transform=axes[0].transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='lightgreen'))\n\n    # Con 1 outlier\n    X_out1 = np.append(X, [9.5])\n    y_out1 = np.append(y, [50])  # Outlier\n    model2 = LinearRegression()\n    model2.fit(X_out1.reshape(-1, 1), y_out1)\n    axes[1].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n    axes[1].scatter([9.5], [50], color='red', s=100, marker='x', linewidth=3)\n    axes[1].plot(X_out1, model2.predict(X_out1.reshape(-1, 1)),\n                color=COLORS['primary'], linewidth=2)\n    plot_style(axes[1], \"Con 1 Outlier\", \"X\", \"Y\")\n    axes[1].text(0.05, 0.95, f'R¬≤ = {model2.score(X_out1.reshape(-1, 1), y_out1):.3f}',\n                transform=axes[1].transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='yellow'))\n\n    # Con 3 outliers\n    X_out3 = np.append(X, [9.5, 8.5, 7.5])\n    y_out3 = np.append(y, [50, 45, 40])  # Outliers\n    model3 = LinearRegression()\n    model3.fit(X_out3.reshape(-1, 1), y_out3)\n    axes[2].scatter(X, y, alpha=0.6, color=COLORS['accent'])\n    axes[2].scatter([9.5, 8.5, 7.5], [50, 45, 40], color='red',\n                   s=100, marker='x', linewidth=3)\n    axes[2].plot(np.sort(X_out3), model3.predict(np.sort(X_out3).reshape(-1, 1)),\n                color=COLORS['primary'], linewidth=2)\n    plot_style(axes[2], \"Con 3 Outliers\", \"X\", \"Y\")\n    axes[2].text(0.05, 0.95, f'R¬≤ = {model3.score(X_out3.reshape(-1, 1), y_out3):.3f}',\n                transform=axes[2].transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='lightcoral'))\n\n    plt.suptitle(\"Impacto de Outliers en Regresi√≥n Lineal\", fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"‚ö†Ô∏è Observaciones:\")\n    print(\"  ‚Ä¢ Los outliers pueden cambiar dram√°ticamente la l√≠nea de regresi√≥n\")\n    print(\"  ‚Ä¢ El R¬≤ disminuye significativamente con outliers\")\n    print(\"  ‚Ä¢ Considerar t√©cnicas robustas (RANSAC, Huber) para datos con outliers\")","metadata":{"id":"-eE1OiqFeF8o"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"NjdMLwmeo7a4"}},{"cell_type":"code","source":"demonstrate_outlier_effect()","metadata":{"id":"kS2ez64AeF8o"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 3. Regresi√≥n Lineal M√∫ltiple\n\n### üìñ Teor√≠a: M√∫ltiples Variables Predictoras\n\nCuando tenemos m√∫ltiples caracter√≠sticas, el modelo se extiende a:\n\n$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\n\nEn forma matricial: $y = X\\theta$\n","metadata":{"id":"P2RyqT9feF8o"}},{"cell_type":"markdown","source":"### üíª Dataset Real: Predicci√≥n de Precios de Casas","metadata":{"id":"6uYr8VuWeF8o"}},{"cell_type":"code","source":"print_section(\"3.1 REGRESI√ìN M√öLTIPLE - DATASET CALIFORNIA HOUSING\")\n\n# Cargar dataset de California Housing\nfrom sklearn.datasets import fetch_california_housing\n\n# Cargar datos\ncalifornia = fetch_california_housing()\nX_calif = pd.DataFrame(california.data, columns=california.feature_names)\ny_calif = california.target\n\nprint(\"üìä Informaci√≥n del Dataset:\")\nprint(f\"  ‚Ä¢ N√∫mero de muestras: {X_calif.shape[0]:,}\")\nprint(f\"  ‚Ä¢ N√∫mero de caracter√≠sticas: {X_calif.shape[1]}\")\nprint(f\"  ‚Ä¢ Variable objetivo: Precio medio de casas (en cientos de miles de d√≥lares)\")\nprint(\"\\nüìã Caracter√≠sticas disponibles:\")\nfor i, col in enumerate(X_calif.columns, 1):\n    print(f\"  {i}. {col}\")\n\n# Estad√≠sticas descriptivas\nprint(\"\\nüìà Estad√≠sticas Descriptivas:\")\ndisplay(X_calif.describe())\n\n# Visualizar distribuciones\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\naxes = axes.ravel()\n\nfor idx, col in enumerate(X_calif.columns):\n    axes[idx].hist(X_calif[col], bins=30, color=COLORS['accent'],\n                   alpha=0.7, edgecolor='black')\n    axes[idx].set_title(col, fontweight='bold')\n    axes[idx].set_ylabel('Frecuencia')\n    axes[idx].grid(True, alpha=0.3)\n\naxes[8].hist(y_calif, bins=30, color=COLORS['success'],\n             alpha=0.7, edgecolor='black')\naxes[8].set_title('Precio (Target)', fontweight='bold')\naxes[8].set_ylabel('Frecuencia')\naxes[8].grid(True, alpha=0.3)\n\nplt.suptitle(\"Distribuci√≥n de Variables - California Housing\",\n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()","metadata":{"id":"g2YXiDpoeF8o"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** \nExploraci√≥n California Housing:\nAqu√≠ se carga y explora el dataset California Housing, que contiene informaci√≥n socioecon√≥mica y geogr√°fica sobre viviendas. Se presentan dimensiones, variables y la variable objetivo (precio medio). Con estad√≠sticas y histogramas se analiza la distribuci√≥n, detectando asimetr√≠as, outliers y diferencias de escala.\n\n\n","metadata":{"id":"LtOaIIM6pLFm"}},{"cell_type":"markdown","source":"### üìä An√°lisis de Correlaci√≥n","metadata":{"id":"KEJ6mTnIeF8p"}},{"cell_type":"code","source":"print_section(\"3.2 AN√ÅLISIS DE CORRELACI√ìN\")\n\n# Crear DataFrame completo\ndf_calif = X_calif.copy()\ndf_calif['Price'] = y_calif\n\n# Matriz de correlaci√≥n\ncorrelation_matrix = df_calif.corr()\n\n# Visualizaci√≥n de correlaci√≥n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Heatmap\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f',\n            cmap='coolwarm', center=0, square=True,\n            linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\naxes[0].set_title('Matriz de Correlaci√≥n', fontsize=14, fontweight='bold')\n\n# Correlaci√≥n con variable objetivo\ntarget_corr = correlation_matrix['Price'].sort_values(ascending=False)[1:]\ncolors = [COLORS['success'] if x > 0 else COLORS['warning'] for x in target_corr]\nbars = axes[1].barh(target_corr.index, target_corr.values, color=colors)\naxes[1].set_xlabel('Correlaci√≥n con Precio')\naxes[1].set_title('Correlaci√≥n de Features con Precio', fontsize=14, fontweight='bold')\naxes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n# A√±adir valores\nfor bar, value in zip(bars, target_corr.values):\n    axes[1].text(value + 0.01 if value > 0 else value - 0.01,\n                bar.get_y() + bar.get_height()/2,\n                f'{value:.3f}', va='center', ha='left' if value > 0 else 'right')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üîç Observaciones clave:\")\nprint(f\"  ‚Ä¢ Mayor correlaci√≥n positiva: {target_corr.index[0]} ({target_corr.values[0]:.3f})\")\nprint(f\"  ‚Ä¢ Mayor correlaci√≥n negativa: {target_corr.index[-1]} ({target_corr.values[-1]:.3f})\")","metadata":{"id":"uJ4n8_BleF8p"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque construye un DataFrame con predictores y precio, y calcula la matriz de correlaci√≥n. El heatmap permite identificar relaciones directas o inversas, redundancias y posibles problemas de multicolinealidad.\n\n\n","metadata":{"id":"N2DyPyfZr9ij"}},{"cell_type":"markdown","source":"### üîß Entrenamiento del Modelo M√∫ltiple","metadata":{"id":"f8QtOnZSeF8p"}},{"cell_type":"code","source":"print_section(\"3.3 ENTRENAMIENTO Y EVALUACI√ìN\")\n\n# Preparar datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X_calif, y_calif, test_size=0.2, random_state=42\n)\n\n# Escalar caracter√≠sticas (importante para regresi√≥n m√∫ltiple)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Entrenar modelo\nmodel_multiple = LinearRegression()\nmodel_multiple.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_train_pred = model_multiple.predict(X_train_scaled)\ny_test_pred = model_multiple.predict(X_test_scaled)\n\n# M√©tricas\ndef evaluate_model(y_true, y_pred, dataset_name):\n    \"\"\"Calcular y mostrar m√©tricas de evaluaci√≥n\"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"\\nüìä M√©tricas para {dataset_name}:\")\n    print(f\"  ‚Ä¢ MSE:  {mse:.4f}\")\n    print(f\"  ‚Ä¢ RMSE: {rmse:.4f}\")\n    print(f\"  ‚Ä¢ MAE:  {mae:.4f}\")\n    print(f\"  ‚Ä¢ R¬≤:   {r2:.4f}\")\n\n    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n\ntrain_metrics = evaluate_model(y_train, y_train_pred, \"Entrenamiento\")\ntest_metrics = evaluate_model(y_test, y_test_pred, \"Prueba\")\n\n# Visualizaci√≥n de predicciones\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Gr√°fico de predicciones vs reales\nfor ax, y_true, y_pred, title, metrics in [\n    (axes[0], y_train, y_train_pred, \"Conjunto de Entrenamiento\", train_metrics),\n    (axes[1], y_test, y_test_pred, \"Conjunto de Prueba\", test_metrics)\n]:\n    ax.scatter(y_true, y_pred, alpha=0.5, s=20, color=COLORS['accent'])\n    ax.plot([y_true.min(), y_true.max()],\n            [y_true.min(), y_true.max()],\n            'r--', linewidth=2, label='Predicci√≥n Perfecta')\n\n    # A√±adir banda de error\n    error_band = 0.5  # ¬±0.5 para visualizaci√≥n\n    ax.fill_between([y_true.min(), y_true.max()],\n                    [y_true.min() - error_band, y_true.max() - error_band],\n                    [y_true.min() + error_band, y_true.max() + error_band],\n                    alpha=0.2, color='gray', label=f'¬±{error_band}')\n\n    plot_style(ax, title, \"Valor Real\", \"Valor Predicho\")\n    ax.legend()\n\n    # A√±adir m√©tricas\n    metrics_text = f\"R¬≤ = {metrics['r2']:.3f}\\nRMSE = {metrics['rmse']:.3f}\"\n    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n           verticalalignment='top',\n           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\nplt.suptitle(\"Evaluaci√≥n del Modelo de Regresi√≥n M√∫ltiple\",\n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()","metadata":{"id":"2_IvU284eF8p"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Se ejecuta el flujo t√≠pico para regresi√≥n m√∫ltiple: divisi√≥n en train/test, estandarizaci√≥n con StandardScaler, ajuste del modelo LinearRegression, predicciones y c√°lculo de m√©tricas (MSE, RMSE, MAE, R¬≤) para evaluar error y capacidad explicativa.","metadata":{"id":"XcdfwoIPsHQk"}},{"cell_type":"markdown","source":"### üìä Importancia de Caracter√≠sticas","metadata":{"id":"yyRDT8Q8eF8p"}},{"cell_type":"code","source":"print_section(\"3.4 IMPORTANCIA DE CARACTER√çSTICAS\")\n\n# Obtener coeficientes\nfeature_importance = pd.DataFrame({\n    'Feature': X_calif.columns,\n    'Coefficient': model_multiple.coef_,\n    'Abs_Coefficient': np.abs(model_multiple.coef_)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(\"üìä Coeficientes del Modelo (ordenados por importancia absoluta):\")\ndisplay(feature_importance)\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Coeficientes\ncolors = [COLORS['success'] if x > 0 else COLORS['warning']\n          for x in feature_importance['Coefficient']]\nbars = axes[0].barh(feature_importance['Feature'],\n                   feature_importance['Coefficient'],\n                   color=colors)\naxes[0].set_xlabel('Coeficiente')\naxes[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\nplot_style(axes[0], \"Coeficientes del Modelo\", \"Valor del Coeficiente\", \"\")\n\n# Importancia absoluta\naxes[1].barh(feature_importance['Feature'],\n            feature_importance['Abs_Coefficient'],\n            color=COLORS['primary'])\naxes[1].set_xlabel('|Coeficiente|')\nplot_style(axes[1], \"Importancia Absoluta\", \"Magnitud\", \"\")\n\nplt.suptitle(\"An√°lisis de Importancia de Caracter√≠sticas\",\n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Interpretaci√≥n:\")\nprint(f\"  ‚Ä¢ Caracter√≠stica m√°s importante: {feature_importance.iloc[0]['Feature']}\")\nprint(f\"    Un aumento de 1 std en {feature_importance.iloc[0]['Feature']} \")\nprint(f\"    cambia el precio en {feature_importance.iloc[0]['Coefficient']:.3f} unidades\")","metadata":{"id":"6-IySybKeF8p"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque analiza la influencia de cada variable en el modelo mediante los coeficientes. Se ordenan por magnitud absoluta y se visualizan con colores para distinguir efectos positivos y negativos, mostrando la fuerza relativa de cada predictor.","metadata":{"id":"YvJhDxP5sgyh"}},{"cell_type":"markdown","source":"---\n\n## 4. Gradient Descent desde Cero\n\n### üìñ Teor√≠a: Optimizaci√≥n Iterativa\n\nEl Gradient Descent encuentra los par√°metros √≥ptimos minimizando iterativamente la funci√≥n de costo:\n\n$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)$$","metadata":{"id":"8cO0Y7FyeF8p"}},{"cell_type":"markdown","source":"### üíª Implementaci√≥n Manual","metadata":{"id":"zmW8yvVBeF8p"}},{"cell_type":"markdown","source":"### Gr√°fica en 3D de nuestra funci√≥n de coste","metadata":{"id":"6Am6o6rHeF8p"}},{"cell_type":"code","source":"from matplotlib import cm # Para manejar colores","metadata":{"id":"kBkdfCN1eF8p"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\ndef f(x,y):\n  return x**2 + y**2;\n\nres = 100\n\nX = np.linspace(-4, 4, res)\nY = np.linspace(-4, 4, res)\n\nX, Y = np.meshgrid(X, Y)\n\nZ = f(X,Y)\n\n# Gr√°ficar la superficie\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.cool,\n                       linewidth=0, antialiased=False)\n\nfig.colorbar(surf)","metadata":{"id":"BXpQFzkreF8p"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Aqu√≠ se genera una visualizaci√≥n tridimensional de una superficie cuadr√°tica. Se crea una malla con np.meshgrid, se calcula Z y se representa con plot_surface usando un mapa de color (cmap=cm.cool).\n\n\n","metadata":{"id":"-s5TwrZis9QJ"}},{"cell_type":"code","source":"level_map = np.linspace(np.min(Z), np.max(Z),res)\nplt.contourf(X, Y, Z, levels=level_map,cmap=cm.cool)\nplt.colorbar()\nplt.title('Descenso del gradiente')\n\ndef derivate(_p,p):\n  return  (f(_p[0],_p[1]) - f(p[0],p[1])) / h\n\np = np.random.rand(2) * 8 - 4 # generar dos valores aleatorios\n\nplt.plot(p[0],p[1],'o', c='k')\n\nlr = 0.01\nh = 0.01\n\ngrad = np.zeros(2)\n\nfor i in range(10000):\n  for idx, val in enumerate(p):\n    _p = np.copy(p)\n\n    _p[idx] = _p[idx] + h;\n\n    dp = derivate(_p,p)\n\n    grad[idx] = dp\n\n  p = p - lr * grad\n\n  if(i % 10 == 0):\n    plt.plot(p[0],p[1],'o', c='r')\n\nplt.plot(p[0],p[1],'o', c='w')\nplt.show()\n\nprint(\"El punto m√≠nimo se encuentra en: \", p)","metadata":{"id":"6XsC9kKheF8q"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque ilustra c√≥mo el descenso de gradiente busca el m√≠nimo global: desde un punto inicial aleatorio, se actualiza iterativamente en direcci√≥n opuesta al gradiente, controlando el paso con la tasa de aprendizaje. Los puntos rojos muestran la trayectoria sobre el mapa de contornos.","metadata":{"id":"3PLAUsJhtT46"}},{"cell_type":"code","source":"print_section(\"4.1 GRADIENT DESCENT - IMPLEMENTACI√ìN DESDE CERO\")\n\nclass GradientDescentRegression:\n    \"\"\"\n    Implementaci√≥n de Regresi√≥n Lineal usando Gradient Descent\n    \"\"\"\n\n    def __init__(self, learning_rate=0.01, n_iterations=1000, verbose=False):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.verbose = verbose\n        self.theta = None\n        self.cost_history = []\n\n    def _add_intercept(self, X):\n        \"\"\"Agregar columna de unos para el intercepto\"\"\"\n        intercept = np.ones((X.shape[0], 1))\n        return np.c_[intercept, X]\n\n    def _compute_cost(self, X, y, theta):\n        \"\"\"Calcular funci√≥n de costo MSE\"\"\"\n        m = len(y)\n        predictions = X.dot(theta)\n        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n        return cost\n\n    def _gradient_descent(self, X, y):\n        \"\"\"Algoritmo de gradient descent\"\"\"\n        m = len(y)\n\n        for i in range(self.n_iterations):\n            # Predicciones actuales\n            predictions = X.dot(self.theta)\n\n            # Calcular errores\n            errors = predictions - y\n\n            # Calcular gradientes\n            gradients = (1 / m) * X.T.dot(errors)\n\n            # Actualizar par√°metros\n            self.theta = self.theta - self.learning_rate * gradients\n\n            # Guardar costo\n            cost = self._compute_cost(X, y, self.theta)\n            self.cost_history.append(cost)\n\n            # Imprimir progreso\n            if self.verbose and i % 100 == 0:\n                print(f\"  Iteraci√≥n {i}: Costo = {cost:.6f}\")\n\n    def fit(self, X, y):\n        \"\"\"Entrenar el modelo\"\"\"\n        # Preparar datos\n        X = np.array(X)\n        y = np.array(y).reshape(-1, 1)\n        X = self._add_intercept(X)\n\n        # Inicializar par√°metros\n        self.theta = np.zeros((X.shape[1], 1))\n\n        # Ejecutar gradient descent\n        self._gradient_descent(X, y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Realizar predicciones\"\"\"\n        X = np.array(X)\n        X = self._add_intercept(X)\n        return X.dot(self.theta).flatten()\n\n    def plot_convergence(self):\n        \"\"\"Visualizar convergencia del algoritmo\"\"\"\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n        # Convergencia completa\n        axes[0].plot(self.cost_history, color=COLORS['primary'], linewidth=2)\n        axes[0].set_xlabel('Iteraci√≥n')\n        axes[0].set_ylabel('Costo J(Œ∏)')\n        plot_style(axes[0], \"Convergencia del Gradient Descent\",\n                  \"Iteraci√≥n\", \"Costo\")\n\n        # Primeras 100 iteraciones\n        axes[1].plot(self.cost_history[:100], color=COLORS['accent'], linewidth=2)\n        axes[1].set_xlabel('Iteraci√≥n')\n        axes[1].set_ylabel('Costo J(Œ∏)')\n        plot_style(axes[1], \"Primeras 100 Iteraciones\",\n                  \"Iteraci√≥n\", \"Costo\")\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"id":"H9cenz_4eF8q"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Aqu√≠ se implementa regresi√≥n lineal desde cero usando descenso de gradiente: se a√±ade intercepto, se inicializan par√°metros, se define la funci√≥n de costo MSE y se actualizan gradientes en cada iteraci√≥n. Se guarda la historia del costo y se visualiza la convergencia para evaluar estabilidad y velocidad.\n\n\n","metadata":{"id":"som0mQBVtocV"}},{"cell_type":"code","source":"# Demostraci√≥n con datos sint√©ticos\nprint(\"üéØ Entrenando modelo con Gradient Descent...\")\nnp.random.seed(42)\nX_gd = np.random.randn(100, 3)  # 3 caracter√≠sticas\ntrue_theta = np.array([2, -1, 0.5, 3])  # Incluye intercepto\ny_gd = X_gd.dot(true_theta[1:]) + true_theta[0] + np.random.randn(100) * 0.5\n\n# Entrenar\ngd_model = GradientDescentRegression(learning_rate=0.1,\n                                     n_iterations=500,\n                                     verbose=True)\ngd_model.fit(X_gd, y_gd)\n\nprint(f\"\\n‚úÖ Par√°metros verdaderos: {true_theta}\")\nprint(f\"‚úÖ Par√°metros encontrados: {gd_model.theta.flatten()}\")\n\n# Visualizar convergencia\ngd_model.plot_convergence()","metadata":{"id":"YXFjKDOJeF8q"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque aplica descenso de gradiente sobre datos sint√©ticos con relaci√≥n lineal conocida, generando tres caracter√≠sticas y una variable objetivo con par√°metros definidos y ruido aleatorio.","metadata":{"id":"sz-Tpau-uKV7"}},{"cell_type":"markdown","source":"### üî¨ Experimento: Efecto del Learning Rate","metadata":{"id":"XpdFQF6ceF8q"}},{"cell_type":"code","source":"print_section(\"4.2 EXPERIMENTO: EFECTO DEL LEARNING RATE\")\n\ndef experiment_learning_rates():\n    \"\"\"Comparar diferentes learning rates\"\"\"\n\n    learning_rates = [0.001, 0.01, 0.1, 0.5]\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.ravel()\n\n    for idx, lr in enumerate(learning_rates):\n        # Entrenar modelo\n        model = GradientDescentRegression(learning_rate=lr,\n                                         n_iterations=200,\n                                         verbose=False)\n        model.fit(X_gd, y_gd)\n\n        # Visualizar\n        ax = axes[idx]\n        ax.plot(model.cost_history, linewidth=2,\n               color=COLORS['accent'] if lr <= 0.1 else COLORS['warning'])\n        ax.set_xlabel('Iteraci√≥n')\n        ax.set_ylabel('Costo J(Œ∏)')\n        ax.set_title(f'Learning Rate = {lr}', fontweight='bold')\n        ax.grid(True, alpha=0.3)\n\n        # A√±adir informaci√≥n\n        if lr <= 0.1:\n            final_cost = model.cost_history[-1]\n            ax.text(0.6, 0.8, f'Converge a {final_cost:.4f}',\n                   transform=ax.transAxes,\n                   bbox=dict(boxstyle='round', facecolor='lightgreen'))\n        else:\n            ax.text(0.6, 0.8, 'Diverge o oscila',\n                   transform=ax.transAxes,\n                   bbox=dict(boxstyle='round', facecolor='lightcoral'))\n\n    plt.suptitle(\"Impacto del Learning Rate en la Convergencia\",\n                fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"üí° Observaciones:\")\n    print(\"  ‚Ä¢ Œ± muy peque√±o (0.001): Convergencia muy lenta\")\n    print(\"  ‚Ä¢ Œ± √≥ptimo (0.01-0.1): Convergencia r√°pida y estable\")\n    print(\"  ‚Ä¢ Œ± muy grande (>0.5): Divergencia o oscilaci√≥n\")","metadata":{"id":"k-MsF-93eF8q"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Se analiza c√≥mo la tasa de aprendizaje (Œ±) afecta el descenso de gradiente. Se prueban distintos valores y se grafican curvas de convergencia para observar estabilidad o divergencia.","metadata":{"id":"0deeWTFXubwa"}},{"cell_type":"code","source":"experiment_learning_rates()","metadata":{"id":"CEQBfg-1eF8q"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 6. Regularizaci√≥n\n\n### üìñ Teor√≠a: Control de Complejidad\n\nLa regularizaci√≥n a√±ade una penalizaci√≥n al costo para evitar overfitting:\n\n- **Ridge (L2)**: $J = MSE + \\lambda \\sum \\theta_i^2$\n- **LASSO (L1)**: $J = MSE + \\lambda \\sum |\\theta_i|$\n- **Elastic Net**: Combina L1 y L2","metadata":{"id":"Q_jmqrOOeF8q"}},{"cell_type":"markdown","source":"### üíª Comparaci√≥n de T√©cnicas de Regularizaci√≥n","metadata":{"id":"FK7DI3OqeF8u"}},{"cell_type":"code","source":"print_section(\"6.1 COMPARACI√ìN DE REGULARIZACI√ìN\")\n\ndef compare_regularization():\n    \"\"\"Comparar Ridge, LASSO y Elastic Net\"\"\"\n\n    # Crear dataset con multicolinealidad\n    np.random.seed(42)\n    n_samples, n_features = 100, 20\n    X_reg = np.random.randn(n_samples, n_features)\n\n    # A√±adir correlaci√≥n entre features\n    X_reg[:, 1] = X_reg[:, 0] + np.random.randn(n_samples) * 0.1\n    X_reg[:, 2] = X_reg[:, 0] + np.random.randn(n_samples) * 0.1\n\n    # Generar y con solo algunas features relevantes\n    true_coef = np.zeros(n_features)\n    true_coef[0:5] = [3, 2, -1.5, 0, -2]  # Solo 5 features relevantes\n    y_reg = X_reg @ true_coef + np.random.randn(n_samples) * 0.5\n\n    # Split datos\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_reg, y_reg, test_size=0.3, random_state=42\n    )\n\n    # Escalar\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Modelos\n    models = {\n        'Linear': LinearRegression(),\n        'Ridge': Ridge(alpha=1.0),\n        'LASSO': Lasso(alpha=0.1),\n        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n    }\n\n    results = {}\n    coefficients = {}\n\n    # Entrenar modelos\n    for name, model in models.items():\n        model.fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n\n        results[name] = {\n            'train_score': model.score(X_train_scaled, y_train),\n            'test_score': model.score(X_test_scaled, y_test),\n            'mse': mean_squared_error(y_test, y_pred),\n            'n_zero_coef': np.sum(np.abs(model.coef_) < 0.01)\n        }\n        coefficients[name] = model.coef_\n\n    # Visualizar resultados\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # 1. Comparaci√≥n de scores\n    models_names = list(results.keys())\n    train_scores = [results[m]['train_score'] for m in models_names]\n    test_scores = [results[m]['test_score'] for m in models_names]\n\n    x = np.arange(len(models_names))\n    width = 0.35\n\n    axes[0, 0].bar(x - width/2, train_scores, width,\n                   label='Train', color=COLORS['primary'])\n    axes[0, 0].bar(x + width/2, test_scores, width,\n                   label='Test', color=COLORS['accent'])\n    axes[0, 0].set_xticks(x)\n    axes[0, 0].set_xticklabels(models_names)\n    axes[0, 0].set_ylim([0, 1])\n    plot_style(axes[0, 0], \"Comparaci√≥n de R¬≤ Score\", \"\", \"R¬≤\")\n    axes[0, 0].legend()\n\n    # 2. Coeficientes\n    for name, coef in coefficients.items():\n        axes[0, 1].plot(coef, 'o-', label=name, alpha=0.7, markersize=4)\n    axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    plot_style(axes[0, 1], \"Valores de Coeficientes\", \"Feature\", \"Coeficiente\")\n    axes[0, 1].legend()\n\n    # 3. N√∫mero de coeficientes cero\n    zero_counts = [results[m]['n_zero_coef'] for m in models_names]\n    axes[1, 0].bar(models_names, zero_counts, color=COLORS['warning'])\n    plot_style(axes[1, 0], \"Coeficientes ‚âà 0 (Sparsity)\",\n              \"Modelo\", \"N√∫mero de Coef ‚âà 0\")\n\n    # A√±adir valores\n    for i, (name, count) in enumerate(zip(models_names, zero_counts)):\n        axes[1, 0].text(i, count + 0.5, str(count), ha='center')\n\n    # 4. MSE Comparison\n    mse_values = [results[m]['mse'] for m in models_names]\n    axes[1, 1].bar(models_names, mse_values, color=COLORS['secondary'])\n    plot_style(axes[1, 1], \"Mean Squared Error\", \"Modelo\", \"MSE\")\n\n    plt.suptitle(\"Comparaci√≥n de T√©cnicas de Regularizaci√≥n\",\n                fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    # Tabla de resultados\n    print(\"\\nüìä Tabla de Resultados:\")\n    results_df = pd.DataFrame(results).T\n    display(results_df)\n\n    print(\"\\nüí° Observaciones:\")\n    print(\"  ‚Ä¢ Linear: Puede overfittear con muchas features\")\n    print(\"  ‚Ä¢ Ridge: Reduce magnitud pero mantiene todas las features\")\n    print(\"  ‚Ä¢ LASSO: Selecciona features (pone algunas en 0)\")\n    print(\"  ‚Ä¢ ElasticNet: Balance entre Ridge y LASSO\")\n\n","metadata":{"id":"CbKEesSFeF8u"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque compara regresi√≥n lineal est√°ndar con m√©todos de regularizaci√≥n en un dataset sint√©tico con multicolinealidad. Se eval√∫an R¬≤, MSE y n√∫mero de coeficientes cercanos a cero, mostrando c√≥mo Ridge reduce magnitudes, Lasso induce sparsity y Elastic Net combina ambos enfoques para mejorar generalizaci√≥n.\n\n\n","metadata":{"id":"sWXLG-qzu5hJ"}},{"cell_type":"code","source":"compare_regularization()","metadata":{"id":"1Idmvu9ceF8u"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 7. Regresi√≥n Log√≠stica\n\n### üìñ Teor√≠a: De Regresi√≥n a Clasificaci√≥n\n\nLa regresi√≥n log√≠stica usa la funci√≥n sigmoide para convertir valores reales en probabilidades:\n\n$$P(y=1|x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$","metadata":{"id":"ECJeOEnpeF8u"}},{"cell_type":"markdown","source":"### üíª Implementaci√≥n desde Cero","metadata":{"id":"7SaGpoVIeF8u"}},{"cell_type":"code","source":"print_section(\"7.1 REGRESI√ìN LOG√çSTICA - IMPLEMENTACI√ìN MANUAL\")\n\nclass LogisticRegressionManual:\n    \"\"\"\n    Implementaci√≥n de Regresi√≥n Log√≠stica usando Gradient Descent\n    \"\"\"\n\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.theta = None\n        self.cost_history = []\n\n    def _sigmoid(self, z):\n        \"\"\"Funci√≥n sigmoide\"\"\"\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip para estabilidad\n\n    def _add_intercept(self, X):\n        \"\"\"Agregar columna de unos\"\"\"\n        intercept = np.ones((X.shape[0], 1))\n        return np.c_[intercept, X]\n\n    def _cost_function(self, X, y, theta):\n        \"\"\"Funci√≥n de costo (cross-entropy)\"\"\"\n        m = len(y)\n        z = X.dot(theta)\n        predictions = self._sigmoid(z)\n\n        # Evitar log(0)\n        epsilon = 1e-7\n        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n\n        cost = -(1/m) * np.sum(y * np.log(predictions) +\n                               (1 - y) * np.log(1 - predictions))\n        return cost\n\n    def fit(self, X, y):\n        \"\"\"Entrenar el modelo\"\"\"\n        X = self._add_intercept(X)\n        m, n = X.shape\n\n        # Inicializar par√°metros\n        self.theta = np.zeros(n)\n\n        # Gradient Descent\n        for i in range(self.n_iterations):\n            # Predicciones\n            z = X.dot(self.theta)\n            predictions = self._sigmoid(z)\n\n            # Gradientes\n            gradients = (1/m) * X.T.dot(predictions - y)\n\n            # Actualizar par√°metros\n            self.theta -= self.learning_rate * gradients\n\n            # Guardar costo\n            cost = self._cost_function(X, y, self.theta)\n            self.cost_history.append(cost)\n\n    def predict_proba(self, X):\n        \"\"\"Predecir probabilidades\"\"\"\n        X = self._add_intercept(X)\n        return self._sigmoid(X.dot(self.theta))\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Predecir clases\"\"\"\n        return (self.predict_proba(X) >= threshold).astype(int)\n\n    def plot_decision_boundary(self, X, y):\n        \"\"\"Visualizar frontera de decisi√≥n (2D)\"\"\"\n        if X.shape[1] != 2:\n            print(\"‚ö†Ô∏è Solo se puede visualizar para 2 features\")\n            return\n\n        fig, ax = plt.subplots(figsize=(10, 8))\n\n        # Plot puntos\n        colors = ['blue' if yi == 0 else 'red' for yi in y]\n        ax.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6,\n                  edgecolors='black', linewidth=0.5)\n\n        # Crear mesh para frontera\n        h = 0.02\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                            np.arange(y_min, y_max, h))\n\n        # Predicciones en mesh\n        Z = self.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n\n        # Contorno\n        ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu', levels=[0, 0.5, 1])\n        ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n\n        plot_style(ax, \"Frontera de Decisi√≥n - Regresi√≥n Log√≠stica\",\n                  \"Feature 1\", \"Feature 2\")\n\n        # Leyenda\n        from matplotlib.patches import Patch\n        legend_elements = [\n            Patch(facecolor='blue', alpha=0.6, label='Clase 0'),\n            Patch(facecolor='red', alpha=0.6, label='Clase 1')\n        ]\n        ax.legend(handles=legend_elements)\n\n        plt.show()","metadata":{"id":"haHGf5MeeF8u"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:**\nAqu√≠ se implementa regresi√≥n log√≠stica desde cero con descenso de gradiente: se a√±ade intercepto, se usa la funci√≥n sigmoide con clipping y se definen m√©todos para probabilidades y clases. Cuando hay dos features, se genera una malla para visualizar la frontera de decisi√≥n.\n\n\n","metadata":{"id":"kpjN7PXjv82C"}},{"cell_type":"code","source":"# Demostraci√≥n con datos sint√©ticos\nprint(\"üéØ Generando dataset de clasificaci√≥n binaria...\")\nnp.random.seed(42)\n\n# Generar dos clases separables\nn_samples = 200\nX_class1 = np.random.randn(n_samples//2, 2) + np.array([2, 2])\nX_class2 = np.random.randn(n_samples//2, 2) + np.array([-2, -2])\nX_log = np.vstack([X_class1, X_class2])\ny_log = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])\n\n# Entrenar modelo\nlog_model = LogisticRegressionManual(learning_rate=0.1, n_iterations=500)\nlog_model.fit(X_log, y_log)\n\nprint(f\"‚úÖ Modelo entrenado\")\nprint(f\"   Par√°metros: Œ∏‚ÇÄ={log_model.theta[0]:.3f}, Œ∏‚ÇÅ={log_model.theta[1]:.3f}, Œ∏‚ÇÇ={log_model.theta[2]:.3f}\")\n\n# Evaluar\ny_pred = log_model.predict(X_log)\naccuracy = np.mean(y_pred == y_log)\nprint(f\"   Accuracy: {accuracy:.2%}\")\n\n# Visualizar frontera\nlog_model.plot_decision_boundary(X_log, y_log)\n\n# Convergencia\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(log_model.cost_history, color=COLORS['primary'], linewidth=2)\nplot_style(ax, \"Convergencia de Regresi√≥n Log√≠stica\", \"Iteraci√≥n\", \"Costo (Cross-Entropy)\")\nplt.show()","metadata":{"id":"ZUe9_eeheF8u"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:**\nEste bloque crea un dataset binario en 2D, entrena la regresi√≥n log√≠stica manual, reporta par√°metros y accuracy, y visualiza la frontera de decisi√≥n junto con la curva de p√©rdida, mostrando c√≥mo la optimizaci√≥n reduce el costo.","metadata":{"id":"gt4tJ-0GwR1H"}},{"cell_type":"markdown","source":"### üî¨ Funci√≥n Sigmoide y Odds","metadata":{"id":"-v2phXPOeF8u"}},{"cell_type":"code","source":"print_section(\"7.2 EXPLORANDO LA FUNCI√ìN SIGMOIDE\")\n\ndef explore_sigmoid():\n    \"\"\"Visualizaci√≥n interactiva de la funci√≥n sigmoide\"\"\"\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # 1. Funci√≥n sigmoide b√°sica\n    z = np.linspace(-10, 10, 1000)\n    sigmoid = 1 / (1 + np.exp(-z))\n\n    axes[0, 0].plot(z, sigmoid, color=COLORS['primary'], linewidth=3)\n    axes[0, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n    axes[0, 0].fill_between(z[z <= 0], 0, sigmoid[z <= 0],\n                           alpha=0.3, color=COLORS['primary'], label='P < 0.5')\n    axes[0, 0].fill_between(z[z >= 0], sigmoid[z >= 0], 1,\n                           alpha=0.3, color=COLORS['accent'], label='P > 0.5')\n    plot_style(axes[0, 0], \"Funci√≥n Sigmoide\", \"z = Œ∏·µÄx\", \"P(y=1)\")\n    axes[0, 0].legend()\n\n    # 2. Derivada de la sigmoide\n    sigmoid_derivative = sigmoid * (1 - sigmoid)\n    axes[0, 1].plot(z, sigmoid_derivative, color=COLORS['secondary'], linewidth=3)\n    axes[0, 1].fill_between(z, 0, sigmoid_derivative, alpha=0.3, color=COLORS['secondary'])\n    plot_style(axes[0, 1], \"Derivada de Sigmoide\", \"z\", \"œÉ'(z) = œÉ(z)(1-œÉ(z))\")\n    axes[0, 1].text(0, 0.22, 'M√°ximo en z=0\\nœÉ\\'(0) = 0.25', ha='center',\n                   bbox=dict(boxstyle='round', facecolor='yellow'))\n\n    # 3. Log-Odds (Logit)\n    p = np.linspace(0.01, 0.99, 100)\n    log_odds = np.log(p / (1 - p))\n\n    axes[1, 0].plot(p, log_odds, color=COLORS['warning'], linewidth=3)\n    axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    axes[1, 0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n    plot_style(axes[1, 0], \"Log-Odds (Logit)\", \"Probabilidad\", \"log(p/(1-p))\")\n    axes[1, 0].set_ylim([-5, 5])\n\n    # 4. Comparaci√≥n de diferentes pendientes\n    z_comp = np.linspace(-5, 5, 100)\n    for beta in [0.5, 1, 2, 4]:\n        sigmoid_beta = 1 / (1 + np.exp(-beta * z_comp))\n        axes[1, 1].plot(z_comp, sigmoid_beta, linewidth=2, label=f'Œ≤={beta}')\n\n    axes[1, 1].axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n    axes[1, 1].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n    plot_style(axes[1, 1], \"Efecto de la Pendiente\", \"z\", \"P(y=1)\")\n    axes[1, 1].legend()\n\n    plt.suptitle(\"Explorando la Funci√≥n Sigmoide\", fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"üí° Observaciones clave:\")\n    print(\"  ‚Ä¢ La sigmoide mapea (-‚àû, +‚àû) ‚Üí (0, 1)\")\n    print(\"  ‚Ä¢ Derivada m√°xima en z=0 (punto de mayor incertidumbre)\")\n    print(\"  ‚Ä¢ Mayor pendiente Œ≤ ‚Üí transici√≥n m√°s abrupta\")\n    print(\"  ‚Ä¢ Log-odds son lineales en las caracter√≠sticas\")","metadata":{"id":"J3v9LMjoeF8u"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Se analiza la sigmoide desde varias perspectivas: su forma b√°sica, c√≥mo transforma valores en probabilidades y su derivada, que indica la regi√≥n de mayor sensibilidad.","metadata":{"id":"U_hCemkSwihN"}},{"cell_type":"code","source":"explore_sigmoid()","metadata":{"id":"KjMHEPLBeF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 8. M√©tricas de Clasificaci√≥n\n\n### üìñ Teor√≠a: Evaluando Clasificadores\n\nM√©tricas clave:\n- **Accuracy**: % de predicciones correctas\n- **Precision**: De los que predije positivo, ¬øcu√°ntos lo eran?\n- **Recall**: De los positivos reales, ¬øcu√°ntos detect√©?\n- **F1-Score**: Media arm√≥nica de precision y recall\n- **AUC-ROC**: √Årea bajo la curva ROC","metadata":{"id":"kxEjY-JZeF8v"}},{"cell_type":"markdown","source":"### üíª An√°lisis Completo de M√©tricas","metadata":{"id":"G2XNz-zBeF8v"}},{"cell_type":"code","source":"print_section(\"8.1 M√âTRICAS DE CLASIFICACI√ìN - DATASET REAL\")\n\n# Cargar dataset de c√°ncer de mama\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\nX_cancer = cancer.data\ny_cancer = cancer.target\n\nprint(\"üìä Dataset de C√°ncer de Mama:\")\nprint(f\"  ‚Ä¢ Muestras: {X_cancer.shape[0]}\")\nprint(f\"  ‚Ä¢ Features: {X_cancer.shape[1]}\")\nprint(f\"  ‚Ä¢ Clases: {cancer.target_names}\")\nprint(f\"  ‚Ä¢ Balance: {np.bincount(y_cancer)}\")\n\n","metadata":{"id":"Tx8MPr3veF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque carga el dataset de c√°ncer de mama de Scikit-learn, mostrando n√∫mero de muestras, caracter√≠sticas y distribuci√≥n de clases.","metadata":{"id":"mdIc6Mh6yFz8"}},{"cell_type":"code","source":"# Split y escalar\nX_train, X_test, y_train, y_test = train_test_split(\n    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Entrenar modelo\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_scaled, y_train)\n\n# Predicciones\ny_pred = log_reg.predict(X_test_scaled)\ny_proba = log_reg.predict_proba(X_test_scaled)[:, 1]","metadata":{"id":"LHRNMFLqeF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Aqu√≠ se realiza el flujo completo para regresi√≥n log√≠stica: divisi√≥n estratificada, escalado, entrenamiento y obtenci√≥n de predicciones y probabilidades.","metadata":{"id":"6IfRv6ISyXn3"}},{"cell_type":"code","source":"def plot_classification_metrics(y_true, y_pred, y_proba):\n    \"\"\"Visualizaci√≥n completa de m√©tricas de clasificaci√≥n\"\"\"\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n    # 1. Matriz de Confusi√≥n\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                cbar=False, ax=axes[0, 0])\n    axes[0, 0].set_xlabel('Predicho')\n    axes[0, 0].set_ylabel('Real')\n    axes[0, 0].set_title('Matriz de Confusi√≥n', fontweight='bold')\n\n    # A√±adir m√©tricas\n    tn, fp, fn, tp = cm.ravel()\n    specificity = tn / (tn + fp)\n    sensitivity = tp / (tp + fn)\n    axes[0, 0].text(2.5, 1, f'Sens: {sensitivity:.2%}\\nSpec: {specificity:.2%}',\n                   bbox=dict(boxstyle='round', facecolor='yellow'))\n\n    # 2. Curva ROC\n    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n    roc_auc = auc(fpr, tpr)\n\n    axes[0, 1].plot(fpr, tpr, color=COLORS['primary'], linewidth=2,\n                   label=f'ROC (AUC = {roc_auc:.3f})')\n    axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n    axes[0, 1].fill_between(fpr, tpr, alpha=0.3, color=COLORS['primary'])\n    axes[0, 1].set_xlabel('False Positive Rate')\n    axes[0, 1].set_ylabel('True Positive Rate')\n    axes[0, 1].set_title('Curva ROC', fontweight='bold')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n\n    # 3. Precision-Recall Curve\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n    avg_precision = average_precision_score(y_true, y_proba)\n\n    axes[0, 2].plot(recall, precision, color=COLORS['accent'], linewidth=2,\n                   label=f'AP = {avg_precision:.3f}')\n    axes[0, 2].fill_between(recall, precision, alpha=0.3, color=COLORS['accent'])\n    axes[0, 2].set_xlabel('Recall')\n    axes[0, 2].set_ylabel('Precision')\n    axes[0, 2].set_title('Curva Precision-Recall', fontweight='bold')\n    axes[0, 2].legend()\n    axes[0, 2].grid(True, alpha=0.3)\n\n    # 4. Distribuci√≥n de Probabilidades\n    axes[1, 0].hist(y_proba[y_true == 0], bins=30, alpha=0.7,\n                   color=COLORS['primary'], label='Clase 0', density=True)\n    axes[1, 0].hist(y_proba[y_true == 1], bins=30, alpha=0.7,\n                   color=COLORS['warning'], label='Clase 1', density=True)\n    axes[1, 0].axvline(x=0.5, color='black', linestyle='--', linewidth=2)\n    axes[1, 0].set_xlabel('Probabilidad Predicha')\n    axes[1, 0].set_ylabel('Densidad')\n    axes[1, 0].set_title('Distribuci√≥n de Probabilidades', fontweight='bold')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # 5. M√©tricas por Umbral\n    thresholds_plot = np.linspace(0.1, 0.9, 50)\n    precisions = []\n    recalls = []\n    f1s = []\n\n    for thresh in thresholds_plot:\n        y_pred_thresh = (y_proba >= thresh).astype(int)\n        if len(np.unique(y_pred_thresh)) > 1:  # Evitar warnings\n            prec = precision_score(y_true, y_pred_thresh, zero_division=0)\n            rec = recall_score(y_true, y_pred_thresh)\n            f1 = f1_score(y_true, y_pred_thresh)\n        else:\n            prec = rec = f1 = 0\n\n        precisions.append(prec)\n        recalls.append(rec)\n        f1s.append(f1)\n\n    axes[1, 1].plot(thresholds_plot, precisions, label='Precision', linewidth=2)\n    axes[1, 1].plot(thresholds_plot, recalls, label='Recall', linewidth=2)\n    axes[1, 1].plot(thresholds_plot, f1s, label='F1-Score', linewidth=2)\n    axes[1, 1].axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n    axes[1, 1].set_xlabel('Umbral')\n    axes[1, 1].set_ylabel('Score')\n    axes[1, 1].set_title('M√©tricas vs Umbral', fontweight='bold')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n\n    # 6. Classification Report\n    from sklearn.metrics import classification_report\n    report = classification_report(y_true, y_pred, output_dict=True)\n\n    # Crear tabla\n    metrics_data = []\n    for label in ['0', '1']:\n        metrics_data.append([\n            f\"Clase {label}\",\n            f\"{report[label]['precision']:.3f}\",\n            f\"{report[label]['recall']:.3f}\",\n            f\"{report[label]['f1-score']:.3f}\",\n            f\"{report[label]['support']:.0f}\"\n        ])\n    metrics_data.append([\n        \"Weighted Avg\",\n        f\"{report['weighted avg']['precision']:.3f}\",\n        f\"{report['weighted avg']['recall']:.3f}\",\n        f\"{report['weighted avg']['f1-score']:.3f}\",\n        f\"{report['weighted avg']['support']:.0f}\"\n    ])\n\n    # Mostrar tabla\n    axes[1, 2].axis('tight')\n    axes[1, 2].axis('off')\n    table = axes[1, 2].table(cellText=metrics_data,\n                            colLabels=['Clase', 'Precision', 'Recall', 'F1', 'Support'],\n                            cellLoc='center',\n                            loc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1.2, 1.5)\n    axes[1, 2].set_title('Classification Report', fontweight='bold', pad=20)\n\n    plt.suptitle(\"An√°lisis Completo de M√©tricas de Clasificaci√≥n\",\n                fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    # Imprimir m√©tricas adicionales\n    print(\"\\nüìä M√©tricas Globales:\")\n    print(f\"  ‚Ä¢ Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n    print(f\"  ‚Ä¢ AUC-ROC: {roc_auc:.3f}\")\n    print(f\"  ‚Ä¢ Average Precision: {avg_precision:.3f}\")","metadata":{"id":"bklFOJg7eF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Aqu√≠ se realiza el flujo completo para regresi√≥n log√≠stica: divisi√≥n estratificada, escalado, entrenamiento y obtenci√≥n de predicciones y probabilidades. ","metadata":{"id":"MZtpJkyKy1f9"}},{"cell_type":"code","source":"plot_classification_metrics(y_test, y_pred, y_proba)","metadata":{"id":"qLE0RmqfeF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 9. Casos Pr√°cticos Reales","metadata":{"id":"IEiNN9MGeF8v"}},{"cell_type":"markdown","source":"### üìä Caso 1: Predicci√≥n de Diabetes","metadata":{"id":"uDmYh-WheF8v"}},{"cell_type":"code","source":"print_section(\"9.1 CASO PR√ÅCTICO: PREDICCI√ìN DE DIABETES\")\n\n# Cargar dataset de diabetes\ndiabetes = load_diabetes()\nX_diabetes = diabetes.data\ny_diabetes = diabetes.target\n\nprint(\"üìä Dataset de Diabetes:\")\nprint(f\"  ‚Ä¢ Muestras: {X_diabetes.shape[0]}\")\nprint(f\"  ‚Ä¢ Features: {X_diabetes.shape[1]}\")\nprint(f\"  ‚Ä¢ Variable objetivo: Progresi√≥n de diabetes (continua)\")\n\n# Informaci√≥n de features\nfeature_names = ['age', 'sex', 'bmi', 'bp', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu']\nprint(\"\\nüìã Features (todas normalizadas):\")\nfor i, name in enumerate(feature_names):\n    print(f\"  {i+1}. {name}\")\n\n# Pipeline completo\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Crear pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', Ridge())\n])\n\n# Grid search para hyperpar√°metros\nparam_grid = {\n    'model__alpha': np.logspace(-3, 3, 20)\n}\n\n# Cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5,\n                          scoring='neg_mean_squared_error',\n                          n_jobs=-1)\n\n# Split datos\nX_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n)\n\n# Entrenar\nprint(\"\\nüîç Realizando Grid Search con Cross-Validation...\")\ngrid_search.fit(X_train_diab, y_train_diab)\n\nprint(f\"\\n‚úÖ Mejores hiperpar√°metros: {grid_search.best_params_}\")\nprint(f\"‚úÖ Mejor score CV: {-grid_search.best_score_:.2f}\")\n\n# Evaluar en test\ny_pred_diab = grid_search.predict(X_test_diab)\ntest_mse = mean_squared_error(y_test_diab, y_pred_diab)\ntest_r2 = r2_score(y_test_diab, y_pred_diab)\n\nprint(f\"\\nüìä Resultados en Test:\")\nprint(f\"  ‚Ä¢ MSE: {test_mse:.2f}\")\nprint(f\"  ‚Ä¢ RMSE: {np.sqrt(test_mse):.2f}\")\nprint(f\"  ‚Ä¢ R¬≤: {test_r2:.3f}\")\n\n# Visualizaci√≥n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# CV scores\ncv_results = pd.DataFrame(grid_search.cv_results_)\naxes[0].plot(cv_results['param_model__alpha'],\n            -cv_results['mean_test_score'],\n            'o-', color=COLORS['primary'])\naxes[0].fill_between(cv_results['param_model__alpha'],\n                     -cv_results['mean_test_score'] - cv_results['std_test_score'],\n                     -cv_results['mean_test_score'] + cv_results['std_test_score'],\n                     alpha=0.3)\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œ± (Regularizaci√≥n)')\naxes[0].set_ylabel('MSE')\naxes[0].axvline(x=grid_search.best_params_['model__alpha'],\n               color='red', linestyle='--')\nplot_style(axes[0], \"Cross-Validation Scores\", \"Œ±\", \"MSE\")\n\n# Predicciones\naxes[1].scatter(y_test_diab, y_pred_diab, alpha=0.6,\n               color=COLORS['accent'], edgecolors='black', linewidth=0.5)\naxes[1].plot([y_test_diab.min(), y_test_diab.max()],\n            [y_test_diab.min(), y_test_diab.max()],\n            'r--', linewidth=2)\nplot_style(axes[1], \"Predicciones en Test\", \"Real\", \"Predicci√≥n\")\n\n# Feature importance\nbest_model = grid_search.best_estimator_['model']\nimportance = np.abs(best_model.coef_)\nsorted_idx = np.argsort(importance)[::-1]\n\naxes[2].barh(range(len(feature_names)), importance[sorted_idx],\n            color=COLORS['secondary'])\naxes[2].set_yticks(range(len(feature_names)))\naxes[2].set_yticklabels([feature_names[i] for i in sorted_idx])\nplot_style(axes[2], \"Importancia de Features\", \"|Coeficiente|\", \"\")\n\nplt.suptitle(\"Predicci√≥n de Progresi√≥n de Diabetes\",\n            fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()","metadata":{"id":"1IabliNMeF8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üè† Caso 2: Sistema Completo de Predicci√≥n de Precios","metadata":{"id":"UTBx49yneF8v"}},{"cell_type":"markdown","source":"**COMENTARIO:** Se desarrolla un caso con el dataset Diabetes: pipeline con estandarizaci√≥n y Ridge, b√∫squeda de hiperpar√°metros con validaci√≥n cruzada, evaluaci√≥n en test y visualizaci√≥n de ajuste y coeficientes.","metadata":{"id":"AdXfuL27y9m9"}},{"cell_type":"code","source":"print_section(\"9.2 SISTEMA COMPLETO: PREDICCI√ìN DE PRECIOS INMOBILIARIOS\")\n\nclass RealEstatePricePredictor:\n    \"\"\"\n    Sistema completo de predicci√≥n de precios con:\n    - Preprocesamiento autom√°tico\n    - Selecci√≥n de features\n    - Multiple models\n    - Interpretabilidad\n    \"\"\"\n\n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.models = {}\n        self.best_model = None\n        self.feature_names = None\n        self.results = {}\n\n    def preprocess(self, X, y=None, fit=True):\n        \"\"\"Preprocesamiento de datos\"\"\"\n        if fit:\n            X_scaled = self.scaler.fit_transform(X)\n        else:\n            X_scaled = self.scaler.transform(X)\n        return X_scaled\n\n    def train_multiple_models(self, X_train, y_train, X_val, y_val):\n        \"\"\"Entrenar y comparar m√∫ltiples modelos\"\"\"\n\n        models_to_try = {\n            'Linear': LinearRegression(),\n            'Ridge': RidgeCV(alphas=np.logspace(-3, 3, 10)),\n            'Lasso': LassoCV(alphas=np.logspace(-3, 1, 10), max_iter=1000),\n            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n            'Polynomial': Pipeline([\n                ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n                ('linear', Ridge(alpha=1))\n            ])\n        }\n\n        for name, model in models_to_try.items():\n            # Entrenar\n            model.fit(X_train, y_train)\n\n            # Predicciones\n            y_train_pred = model.predict(X_train)\n            y_val_pred = model.predict(X_val)\n\n            # M√©tricas\n            self.results[name] = {\n                'model': model,\n                'train_r2': r2_score(y_train, y_train_pred),\n                'val_r2': r2_score(y_val, y_val_pred),\n                'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n                'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred))\n            }\n\n        # Seleccionar mejor modelo\n        best_name = max(self.results.keys(),\n                       key=lambda k: self.results[k]['val_r2'])\n        self.best_model = self.results[best_name]['model']\n\n        return self.results\n\n    def plot_results(self):\n        \"\"\"Visualizar comparaci√≥n de modelos\"\"\"\n\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # Preparar datos\n        model_names = list(self.results.keys())\n        train_r2 = [self.results[m]['train_r2'] for m in model_names]\n        val_r2 = [self.results[m]['val_r2'] for m in model_names]\n        train_rmse = [self.results[m]['train_rmse'] for m in model_names]\n        val_rmse = [self.results[m]['val_rmse'] for m in model_names]\n\n        # R¬≤ Comparison\n        x = np.arange(len(model_names))\n        width = 0.35\n\n        axes[0, 0].bar(x - width/2, train_r2, width, label='Train',\n                      color=COLORS['primary'])\n        axes[0, 0].bar(x + width/2, val_r2, width, label='Validation',\n                      color=COLORS['accent'])\n        axes[0, 0].set_xticks(x)\n        axes[0, 0].set_xticklabels(model_names, rotation=45)\n        axes[0, 0].set_ylim([0, 1])\n        plot_style(axes[0, 0], \"R¬≤ Score Comparison\", \"\", \"R¬≤\")\n        axes[0, 0].legend()\n\n        # RMSE Comparison\n        axes[0, 1].bar(x - width/2, train_rmse, width, label='Train',\n                      color=COLORS['primary'])\n        axes[0, 1].bar(x + width/2, val_rmse, width, label='Validation',\n                      color=COLORS['accent'])\n        axes[0, 1].set_xticks(x)\n        axes[0, 1].set_xticklabels(model_names, rotation=45)\n        plot_style(axes[0, 1], \"RMSE Comparison\", \"\", \"RMSE\")\n        axes[0, 1].legend()\n\n        # Overfitting analysis\n        overfitting = np.array(train_r2) - np.array(val_r2)\n        colors = [COLORS['success'] if o < 0.1 else COLORS['warning']\n                 if o < 0.2 else COLORS['warning'] for o in overfitting]\n        axes[1, 0].bar(model_names, overfitting, color=colors)\n        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n        axes[1, 0].set_xticklabels(model_names, rotation=45)\n        plot_style(axes[1, 0], \"Overfitting Analysis\", \"\", \"Train R¬≤ - Val R¬≤\")\n\n        # Summary table\n        axes[1, 1].axis('tight')\n        axes[1, 1].axis('off')\n\n        table_data = []\n        for name in model_names:\n            table_data.append([\n                name,\n                f\"{self.results[name]['train_r2']:.3f}\",\n                f\"{self.results[name]['val_r2']:.3f}\",\n                f\"{self.results[name]['val_rmse']:.1f}\"\n            ])\n\n        table = axes[1, 1].table(cellText=table_data,\n                                colLabels=['Modelo', 'Train R¬≤', 'Val R¬≤', 'Val RMSE'],\n                                cellLoc='center',\n                                loc='center')\n        table.auto_set_font_size(False)\n        table.set_fontsize(11)\n        table.scale(1.2, 2)\n\n        # Highlight best model\n        best_idx = val_r2.index(max(val_r2))\n        for j in range(4):\n            table[(best_idx + 1, j)].set_facecolor('#90EE90')\n\n        plt.suptitle(\"Comparaci√≥n de Modelos de Regresi√≥n\",\n                    fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n# Aplicar sistema\nprint(\"üèóÔ∏è Creando sistema de predicci√≥n de precios...\")\n\n# Usar California Housing\nX_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(\n    X_calif, y_calif, test_size=0.2, random_state=42\n)\n\n# Crear validaci√≥n set\nX_train_final, X_val_cal, y_train_final, y_val_cal = train_test_split(\n    X_train_cal, y_train_cal, test_size=0.2, random_state=42\n)\n\n# Inicializar sistema\npredictor = RealEstatePricePredictor()\npredictor.feature_names = X_calif.columns\n\n# Preprocesar\nX_train_proc = predictor.preprocess(X_train_final)\nX_val_proc = predictor.preprocess(X_val_cal, fit=False)\n\n# Entrenar modelos\nprint(\"üéØ Entrenando m√∫ltiples modelos...\")\nresults = predictor.train_multiple_models(X_train_proc, y_train_final,\n                                         X_val_proc, y_val_cal)\n\n# Visualizar\npredictor.plot_results()\n\n# Mejor modelo\nbest_model_name = max(results.keys(), key=lambda k: results[k]['val_r2'])\nprint(f\"\\nüèÜ Mejor modelo: {best_model_name}\")\nprint(f\"   Val R¬≤: {results[best_model_name]['val_r2']:.3f}\")\nprint(f\"   Val RMSE: {results[best_model_name]['val_rmse']:.3f}\")","metadata":{"id":"ckb3nwjzeF8w"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Este bloque arma un flujo modular para predecir precios con California Housing: preprocesamiento, entrenamiento de varios modelos (lineal, Ridge, Lasso, Elastic Net, polin√≥mico), comparaci√≥n de m√©tricas y selecci√≥n del mejor modelo.\n\n\n","metadata":{"id":"Pi0ggWnMzcmg"}},{"cell_type":"markdown","source":"---\n\n## 10. Proyecto Final Integrador\n\n### üéØ Proyecto: Sistema de Scoring de Cr√©dito","metadata":{"id":"O1_AsUiUeF8w"}},{"cell_type":"code","source":"print_section(\"10. PROYECTO FINAL: SISTEMA DE SCORING DE CR√âDITO\")\n\nprint(\"\"\"\nüìã DESCRIPCI√ìN DEL PROYECTO:\nConstruir un sistema completo de scoring crediticio que:\n1. Prediga la probabilidad de default de un cliente\n2. Asigne un score de cr√©dito (300-850)\n3. Determine el l√≠mite de cr√©dito apropiado\n4. Proporcione explicaciones interpretables\n\"\"\")\n\n# Generar dataset sint√©tico de cr√©dito\nnp.random.seed(42)\nn_samples = 5000\n\n# Features\ncredit_data = pd.DataFrame({\n    'age': np.random.normal(40, 15, n_samples).clip(18, 80),\n    'income': np.random.lognormal(10.5, 0.8, n_samples),\n    'employment_years': np.random.exponential(5, n_samples).clip(0, 40),\n    'debt_to_income': np.random.beta(2, 5, n_samples),\n    'credit_inquiries': np.random.poisson(2, n_samples),\n    'credit_accounts': np.random.poisson(5, n_samples) + 1,\n    'missed_payments': np.random.poisson(0.5, n_samples),\n    'credit_utilization': np.random.beta(2, 3, n_samples),\n    'bankruptcy': np.random.binomial(1, 0.05, n_samples),\n    'home_owner': np.random.binomial(1, 0.6, n_samples)\n})\n\n# Target (probabilidad base de default)\ndefault_prob_base = (\n    0.5 * credit_data['debt_to_income'] +\n    0.3 * credit_data['credit_utilization'] +\n    0.1 * (credit_data['missed_payments'] / 10) +\n    0.2 * credit_data['bankruptcy'] -\n    0.1 * (credit_data['income'] / credit_data['income'].max()) -\n    0.1 * credit_data['home_owner'] +\n    np.random.normal(0, 0.1, n_samples)\n)\n\n# Convertir a probabilidad y luego a binario\ndefault_prob = 1 / (1 + np.exp(-3 * (default_prob_base - 0.3)))\ncredit_data['default'] = (default_prob > np.random.random(n_samples)).astype(int)\n\nprint(\"üìä Dataset de Cr√©dito Generado:\")\nprint(f\"  ‚Ä¢ Muestras: {len(credit_data):,}\")\nprint(f\"  ‚Ä¢ Features: {len(credit_data.columns) - 1}\")\nprint(f\"  ‚Ä¢ Tasa de default: {credit_data['default'].mean():.1%}\")\n\n# Exploraci√≥n inicial\nprint(\"\\nüìà Estad√≠sticas Descriptivas:\")\ndisplay(credit_data.describe())\n\n# An√°lisis de default por caracter√≠stica\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfeatures_to_analyze = ['age', 'income', 'debt_to_income',\n                       'credit_utilization', 'missed_payments', 'credit_inquiries']\n\nfor idx, feature in enumerate(features_to_analyze):\n    # Crear bins para an√°lisis\n    if feature in ['age', 'income']:\n        bins = pd.qcut(credit_data[feature], q=5, duplicates='drop')\n    else:\n        bins = pd.cut(credit_data[feature], bins=5)\n\n    # Calcular tasa de default por bin\n    default_rate = credit_data.groupby(bins)['default'].mean()\n\n    axes[idx].bar(range(len(default_rate)), default_rate.values,\n                 color=COLORS['warning'], alpha=0.7)\n    axes[idx].set_xlabel(feature)\n    axes[idx].set_ylabel('Tasa de Default')\n    axes[idx].set_title(f'Default vs {feature}', fontweight='bold')\n    axes[idx].grid(True, alpha=0.3)\n\n    # Rotar labels si es necesario\n    if idx < 2:\n        axes[idx].set_xticklabels([f'{int(b.left)}-{int(b.right)}'\n                                   for b in default_rate.index], rotation=45)\n\nplt.suptitle(\"An√°lisis Exploratorio de Default\", fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nclass CreditScoringSystem:\n    \"\"\"\n    Sistema completo de scoring crediticio\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.scaler = StandardScaler()\n        self.feature_importance = None\n\n    def prepare_features(self, data):\n        \"\"\"Ingenier√≠a de caracter√≠sticas\"\"\"\n        features = data.copy()\n\n        # Crear nuevas features\n        features['income_per_year'] = features['income'] / (features['age'] - 18 + 1)\n        features['payment_history_score'] = (\n            1 - features['missed_payments'] / (features['missed_payments'].max() + 1)\n        )\n        features['credit_age'] = features['employment_years'] * features['credit_accounts']\n        features['financial_stress'] = (\n            features['debt_to_income'] * features['credit_utilization']\n        )\n\n        # Eliminar target si existe\n        if 'default' in features.columns:\n            features = features.drop('default', axis=1)\n\n        return features\n\n    def train(self, X_train, y_train):\n        \"\"\"Entrenar modelo de scoring\"\"\"\n\n        # Preparar features\n        X_prep = self.prepare_features(X_train)\n\n        # Escalar\n        X_scaled = self.scaler.fit_transform(X_prep)\n\n        # Entrenar modelo con regularizaci√≥n\n        self.model = LogisticRegression(\n            class_weight='balanced',\n            penalty='l2',\n            C=0.1,\n            max_iter=1000\n        )\n        self.model.fit(X_scaled, y_train)\n\n        # Guardar importancia\n        self.feature_importance = pd.DataFrame({\n            'feature': X_prep.columns,\n            'importance': np.abs(self.model.coef_[0])\n        }).sort_values('importance', ascending=False)\n\n        return self\n\n    def predict_default_probability(self, X):\n        \"\"\"Predecir probabilidad de default\"\"\"\n        X_prep = self.prepare_features(X)\n        X_scaled = self.scaler.transform(X_prep)\n        return self.model.predict_proba(X_scaled)[:, 1]\n\n    def calculate_credit_score(self, default_prob):\n        \"\"\"Convertir probabilidad en score (300-850)\"\"\"\n        # Mapeo inverso: menor probabilidad = mayor score\n        # Usando funci√≥n log√≠stica inversa\n        score = 300 + 550 * (1 - default_prob) ** 2\n        return np.clip(score, 300, 850).astype(int)\n\n    def determine_credit_limit(self, income, score):\n        \"\"\"Determinar l√≠mite de cr√©dito basado en income y score\"\"\"\n        # Factor basado en score\n        score_factor = (score - 300) / 550\n\n        # L√≠mite base: 20-50% del ingreso anual\n        base_limit = income * (0.2 + 0.3 * score_factor)\n\n        # Ajustar por categor√≠as de score\n        if score >= 750:\n            multiplier = 1.5\n        elif score >= 700:\n            multiplier = 1.2\n        elif score >= 650:\n            multiplier = 1.0\n        elif score >= 600:\n            multiplier = 0.7\n        else:\n            multiplier = 0.3\n\n        return int(base_limit * multiplier)\n\n    def generate_report(self, customer_data):\n        \"\"\"Generar reporte completo de cr√©dito\"\"\"\n\n        # Predicci√≥n\n        default_prob = self.predict_default_probability(customer_data)[0]\n        credit_score = self.calculate_credit_score(default_prob)\n        credit_limit = self.determine_credit_limit(\n            customer_data['income'].values[0],\n            credit_score\n        )\n\n        # Categor√≠a de riesgo\n        if credit_score >= 750:\n            risk_category = \"Excelente\"\n            risk_color = \"green\"\n        elif credit_score >= 700:\n            risk_category = \"Bueno\"\n            risk_color = \"lightgreen\"\n        elif credit_score >= 650:\n            risk_category = \"Regular\"\n            risk_color = \"yellow\"\n        elif credit_score >= 600:\n            risk_category = \"Malo\"\n            risk_color = \"orange\"\n        else:\n            risk_category = \"Muy Malo\"\n            risk_color = \"red\"\n\n        # Factores principales\n        X_prep = self.prepare_features(customer_data)\n        X_scaled = self.scaler.transform(X_prep)\n        contributions = X_scaled[0] * self.model.coef_[0]\n\n        factors = pd.DataFrame({\n            'Factor': X_prep.columns,\n            'Valor': X_prep.values[0],\n            'Impacto': contributions\n        }).sort_values('Impacto', key=abs, ascending=False).head(5)\n\n        return {\n            'credit_score': credit_score,\n            'default_probability': default_prob,\n            'risk_category': risk_category,\n            'risk_color': risk_color,\n            'credit_limit': credit_limit,\n            'main_factors': factors,\n            'recommendation': self._generate_recommendation(credit_score, factors)\n        }\n\n    def _generate_recommendation(self, score, factors):\n        \"\"\"Generar recomendaciones personalizadas\"\"\"\n        recommendations = []\n\n        if score < 650:\n            recommendations.append(\"‚ö†Ô∏è Score bajo requiere mejoras urgentes\")\n\n            # Analizar factores negativos\n            negative_factors = factors[factors['Impacto'] > 0].head(3)\n            for _, factor in negative_factors.iterrows():\n                if 'missed_payments' in factor['Factor']:\n                    recommendations.append(\"üìå Reducir pagos atrasados\")\n                elif 'debt_to_income' in factor['Factor']:\n                    recommendations.append(\"üìå Reducir ratio deuda/ingreso\")\n                elif 'credit_utilization' in factor['Factor']:\n                    recommendations.append(\"üìå Usar menos del 30% del cr√©dito disponible\")\n        else:\n            recommendations.append(\"‚úÖ Score aceptable, mantener buen comportamiento\")\n\n        return recommendations\n\n    def plot_report(self, report):\n        \"\"\"Visualizar reporte de cr√©dito\"\"\"\n\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # 1. Score Gauge\n        ax = axes[0, 0]\n        ax.axis('equal')\n\n        # Crear gauge\n        theta = np.linspace(np.pi, 0, 100)\n        r_inner = 0.7\n        r_outer = 1.0\n\n        # Colores por segmento\n        colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n        bounds = [300, 600, 650, 700, 750, 850]\n\n        for i in range(len(colors)):\n            start_angle = np.pi - (bounds[i] - 300) / 550 * np.pi\n            end_angle = np.pi - (bounds[i+1] - 300) / 550 * np.pi\n            theta_seg = np.linspace(start_angle, end_angle, 20)\n\n            x_inner = r_inner * np.cos(theta_seg)\n            y_inner = r_inner * np.sin(theta_seg)\n            x_outer = r_outer * np.cos(theta_seg)\n            y_outer = r_outer * np.sin(theta_seg)\n\n            verts = list(zip(x_outer, y_outer)) + list(zip(x_inner[::-1], y_inner[::-1]))\n            poly = plt.Polygon(verts, facecolor=colors[i], edgecolor='white', linewidth=2)\n            ax.add_patch(poly)\n\n        # Aguja del score\n        score_angle = np.pi - (report['credit_score'] - 300) / 550 * np.pi\n        ax.arrow(0, 0, 0.9 * np.cos(score_angle), 0.9 * np.sin(score_angle),\n                head_width=0.05, head_length=0.05, fc='black', ec='black', linewidth=2)\n\n        # Centro\n        circle = plt.Circle((0, 0), 0.05, color='black')\n        ax.add_patch(circle)\n\n        # Texto\n        ax.text(0, -0.3, f\"{report['credit_score']}\",\n               fontsize=36, fontweight='bold', ha='center')\n        ax.text(0, -0.45, report['risk_category'],\n               fontsize=16, ha='center', color=report['risk_color'])\n\n        ax.set_xlim(-1.2, 1.2)\n        ax.set_ylim(-0.6, 1.2)\n        ax.axis('off')\n        ax.set_title('Credit Score', fontsize=14, fontweight='bold')\n\n        # 2. Probabilidad de Default\n        ax = axes[0, 1]\n        prob = report['default_probability']\n\n        # Barra de probabilidad\n        ax.barh(['Probabilidad\\nde Default'], [prob], color='red', alpha=0.7)\n        ax.barh(['Probabilidad\\nde No Default'], [1-prob], left=[prob], color='green', alpha=0.7)\n        ax.set_xlim([0, 1])\n        ax.set_xlabel('Probabilidad')\n        ax.set_title('Riesgo de Default', fontsize=14, fontweight='bold')\n\n        # Texto con porcentaje\n        ax.text(prob/2, 0, f'{prob:.1%}', ha='center', va='center',\n               fontsize=12, fontweight='bold', color='white')\n        ax.text(prob + (1-prob)/2, 0, f'{1-prob:.1%}', ha='center', va='center',\n               fontsize=12, fontweight='bold', color='white')\n\n        # 3. Factores principales\n        ax = axes[1, 0]\n        factors = report['main_factors']\n        colors_factors = ['red' if x > 0 else 'green' for x in factors['Impacto']]\n\n        ax.barh(factors['Factor'][:5], factors['Impacto'][:5], color=colors_factors)\n        ax.set_xlabel('Impacto en Score')\n        ax.set_title('Factores Principales', fontsize=14, fontweight='bold')\n        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n        # 4. Resumen y recomendaciones\n        ax = axes[1, 1]\n        ax.axis('off')\n\n        summary_text = f\"\"\"\nüìä RESUMEN DE CR√âDITO\n\nüí≥ L√≠mite Aprobado: ${report['credit_limit']:,}\n\nüìà Categor√≠a de Riesgo: {report['risk_category']}\n\nüéØ Probabilidad de Default: {report['default_probability']:.1%}\n\nüìã RECOMENDACIONES:\n\"\"\"\n        for rec in report['recommendation']:\n            summary_text += f\"\\n{rec}\"\n\n        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,\n               fontsize=11, verticalalignment='top',\n               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n\n        plt.suptitle(\"Reporte de Scoring Crediticio\", fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n# Entrenar sistema\nprint(\"\\nüéØ Entrenando Sistema de Scoring...\")\n\n# Split datos\nX = credit_data.drop('default', axis=1)\ny = credit_data['default']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Crear y entrenar sistema\nscoring_system = CreditScoringSystem()\nscoring_system.train(X_train, y_train)\n\n# Evaluar\ny_pred_proba = scoring_system.predict_default_probability(X_test)\ny_pred = (y_pred_proba > 0.5).astype(int)\n\nprint(\"\\nüìä Evaluaci√≥n del Sistema:\")\nprint(f\"  ‚Ä¢ Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(f\"  ‚Ä¢ AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\nprint(f\"  ‚Ä¢ Precision: {precision_score(y_test, y_pred):.3f}\")\nprint(f\"  ‚Ä¢ Recall: {recall_score(y_test, y_pred):.3f}\")\n\n# Generar reporte para un cliente ejemplo\nprint(\"\\nüìã Generando reporte para cliente ejemplo...\")\nsample_customer = X_test.iloc[[0]]\nreport = scoring_system.generate_report(sample_customer)\n\nprint(f\"\\nüéØ Resultado para Cliente:\")\nprint(f\"  ‚Ä¢ Credit Score: {report['credit_score']}\")\nprint(f\"  ‚Ä¢ Categor√≠a: {report['risk_category']}\")\nprint(f\"  ‚Ä¢ L√≠mite de Cr√©dito: ${report['credit_limit']:,}\")\n\n# Visualizar reporte\nscoring_system.plot_report(report)\n\n# Feature importance global\nfig, ax = plt.subplots(figsize=(10, 6))\ntop_features = scoring_system.feature_importance.head(10)\nax.barh(top_features['feature'], top_features['importance'], color=COLORS['primary'])\nax.set_xlabel('Importancia (|Coeficiente|)')\nax.set_title('Importancia Global de Features en Scoring', fontweight='bold')\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéì PROYECTO COMPLETADO\")\nprint(\"=\"*60)\nprint(\"\"\"\n‚úÖ Has implementado exitosamente:\n1. Regresi√≥n Lineal desde cero\n2. Gradient Descent manual\n3. Diagn√≥stico completo de modelos\n4. T√©cnicas de regularizaci√≥n\n5. Regresi√≥n Log√≠stica desde cero\n6. Manejo de clases desbalanceadas\n7. Sistema completo de scoring crediticio\n\nüöÄ Pr√≥ximos pasos sugeridos:\n- Experimentar con m√°s datasets reales\n- Implementar validaci√≥n cruzada temporal\n- A√±adir interpretabilidad (SHAP, LIME)\n- Desplegar modelo en producci√≥n\n- Monitorear drift y performance\n\n¬°Felicitaciones por completar el notebook! üéâ\n\"\"\")","metadata":{"id":"FKNGKs-meF8w"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**COMENTARIO:** Aqu√≠ se construye un sistema end-to-end para scoring crediticio con datos sint√©ticos: generaci√≥n de dataset, an√°lisis, ingenier√≠a de caracter√≠sticas, clasificaci√≥n log√≠stica con balanceo, conversi√≥n a score, c√°lculo de l√≠mites y visualizaciones interpretables, adem√°s de m√©tricas est√°ndar.","metadata":{"id":"nr91Koqt0bxy"}},{"cell_type":"markdown","source":"---\n\n## üéØ Conclusi√≥n\n\nEste notebook ha cubierto exhaustivamente los conceptos de **Regresi√≥n Lineal y Log√≠stica**, desde implementaciones manuales hasta sistemas completos de producci√≥n. Has aprendido:\n\n1. **Fundamentos matem√°ticos** y su implementaci√≥n pr√°ctica\n2. **Diagn√≥stico y evaluaci√≥n** de modelos\n3. **T√©cnicas de regularizaci√≥n** para evitar overfitting\n4. **Manejo de problemas reales** como desbalance y multicolinealidad\n5. **Construcci√≥n de sistemas completos** de ML","metadata":{"id":"FiDfypIOeF8w"}},{"cell_type":"code","source":"from datetime import datetime\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üèÜ CERTIFICADO DE COMPLETACI√ìN\")\nprint(\"=\"*60)\nprint(f\"\"\"\nFecha: {datetime.now().strftime(\"%d de %B, %Y\")}\n\nFelicitaciones por completar exitosamente el notebook\n\"Regresi√≥n Lineal y Log√≠stica: Gu√≠a Completa\"\n\nHas demostrado dominio en:\n‚úì Implementaci√≥n de algoritmos desde cero\n‚úì Uso de librer√≠as de Machine Learning\n‚úì Diagn√≥stico y optimizaci√≥n de modelos\n‚úì Resoluci√≥n de problemas pr√°cticos\n‚úì Construcci√≥n de sistemas de producci√≥n\n\n¬°Contin√∫a tu viaje en Machine Learning! üöÄ\n\"\"\")\nprint(\"=\"*60)","metadata":{"id":"COmI9rrjeF8w"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n**Fin del Notebook**\n\n*√öltima actualizaci√≥n: 2025*  \n*Autor: Adaptado y mejorado para el curso de Machine Learning*","metadata":{"id":"cDyN583veF8w"}},{"cell_type":"code","source":"","metadata":{"id":"kTMY6n5neF8w"},"outputs":[],"execution_count":null}]}